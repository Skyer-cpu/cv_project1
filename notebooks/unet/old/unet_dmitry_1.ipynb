{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef663154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "from torch.nn.functional import relu\n",
    "from torchmetrics.classification import PrecisionRecallCurve\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Для более сложных аугментаций рекомендуется использовать библиотеку Albumentations\n",
    "import albumentations as A\n",
    "from albumentations import Compose, VerticalFlip, RandomRotate90, Affine\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# позволяет удобно отображать прогресс выполнения циклов \n",
    "# и других длительных операций прямо в консоли или Jupyter Notebook\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a1702d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/pampa89d/.cache/kagglehub/datasets/quadeer15sh/augmented-forest-segmentation/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"quadeer15sh/augmented-forest-segmentation\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21223b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# ускорить подбор оптимальных алгоритмов свёрток\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.enabled = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddded078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (e11): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e12): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e21): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e22): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e31): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e32): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e41): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e42): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (e51): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (e52): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv1): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d11): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv2): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d21): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d22): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d31): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d32): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (upconv4): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "  (d41): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (d42): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (outconv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "\n",
    "        # ENCODER\n",
    "        # In the encoder, convolutional layers with the Conv2d function are used to extract features from the input image. \n",
    "        # Each block in the encoder consists of two convolutional layers followed by a max-pooling layer, with the exception of the last block which does not include a max-pooling layer.\n",
    "        # -------\n",
    "        # input: 572x572x3\n",
    "        self.e11 = nn.Conv2d(3, 64, kernel_size=3, padding=1) # output: 570x570x64\n",
    "        self.e12 = nn.Conv2d(64, 64, kernel_size=3, padding=1) # output: 568x568x64\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 284x284x64\n",
    "\n",
    "        # input: 284x284x64\n",
    "        self.e21 = nn.Conv2d(64, 128, kernel_size=3, padding=1) # output: 282x282x128\n",
    "        self.e22 = nn.Conv2d(128, 128, kernel_size=3, padding=1) # output: 280x280x128\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 140x140x128\n",
    "\n",
    "        # input: 140x140x128\n",
    "        self.e31 = nn.Conv2d(128, 256, kernel_size=3, padding=1) # output: 138x138x256\n",
    "        self.e32 = nn.Conv2d(256, 256, kernel_size=3, padding=1) # output: 136x136x256\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 68x68x256\n",
    "\n",
    "        # input: 68x68x256\n",
    "        self.e41 = nn.Conv2d(256, 512, kernel_size=3, padding=1) # output: 66x66x512\n",
    "        self.e42 = nn.Conv2d(512, 512, kernel_size=3, padding=1) # output: 64x64x512\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2) # output: 32x32x512\n",
    "\n",
    "        # input: 32x32x512\n",
    "        self.e51 = nn.Conv2d(512, 1024, kernel_size=3, padding=1) # output: 30x30x1024\n",
    "        self.e52 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1) # output: 28x28x1024\n",
    "\n",
    "        # DECODER\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.d41 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.d42 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(64, n_class, kernel_size=1)\n",
    "\n",
    "    # определяет последовательность прохождения данных через слои U-Net\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.e12(self.e11(x))\n",
    "        p1 = self.pool1(x1)\n",
    "        x2 = self.e22(self.e21(p1))\n",
    "        p2 = self.pool2(x2)\n",
    "        x3 = self.e32(self.e31(p2))\n",
    "        p3 = self.pool3(x3)\n",
    "        x4 = self.e42(self.e41(p3))\n",
    "        p4 = self.pool4(x4)\n",
    "        x5 = self.e52(self.e51(p4))\n",
    "\n",
    "        # Decoder\n",
    "        u1 = self.upconv1(x5)\n",
    "        c1 = torch.cat([u1, x4], dim=1)\n",
    "        d1 = self.d12(self.d11(c1))\n",
    "\n",
    "        u2 = self.upconv2(d1)\n",
    "        c2 = torch.cat([u2, x3], dim=1)\n",
    "        d2 = self.d22(self.d21(c2))\n",
    "\n",
    "        u3 = self.upconv3(d2)\n",
    "        c3 = torch.cat([u3, x2], dim=1)\n",
    "        d3 = self.d32(self.d31(c3))\n",
    "\n",
    "        u4 = self.upconv4(d3)\n",
    "        c4 = torch.cat([u4, x1], dim=1)\n",
    "        d4 = self.d42(self.d41(c4))\n",
    "\n",
    "        out = self.outconv(d4)\n",
    "        return out\n",
    "\n",
    "# 1 класс - объект, 0 - пусто\n",
    "model = UNet(n_class=1)\n",
    "model.to(device=DEVICE, memory_format=torch.channels_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d60fc61",
   "metadata": {},
   "source": [
    "# 1. Подготовка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e02cad98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Обычно рекомендуется хранить изображения и соответствующие маски в отдельных папках \\nс одинаковыми именами файлов.\\n\\ndataset/\\n  images/\\n    img1.png\\n    img2.png\\n    ...\\n  masks/\\n    img1.png\\n    img2.png\\n    ...\\n    \\nТребования:\\n- Все изображения и маски должны быть одинакового размера.\\n- Маски обычно бинарные (0 — фон, 1 или 255 — объект) или многоклассовые.\\n- Цветовое пространство изображений — обычно RGB, масок — одноканальное.\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Обычно рекомендуется хранить изображения и соответствующие маски в отдельных папках \n",
    "с одинаковыми именами файлов.\n",
    "\n",
    "dataset/\n",
    "  images/\n",
    "    img1.png\n",
    "    img2.png\n",
    "    ...\n",
    "  masks/\n",
    "    img1.png\n",
    "    img2.png\n",
    "    ...\n",
    "    \n",
    "Требования:\n",
    "- Все изображения и маски должны быть одинакового размера.\n",
    "- Маски обычно бинарные (0 — фон, 1 или 255 — объект) или многоклассовые.\n",
    "- Цветовое пространство изображений — обычно RGB, масок — одноканальное.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c347b25",
   "metadata": {},
   "source": [
    "# 2. Предобработка и загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcff609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- Аугментации: Легкие трансформации (повороты, флипы, масштабирование) улучшают обобщаемость модели.\\n\\n- Класс Dataset: В PyTorch или tf.data (TensorFlow) реализуйте пользовательский загрузчик, \\n    который синхронно загружает изображения и соответствующие маски,\\n    преобразует их в тензоры, нормализует и, при необходимости, применяет аугментации.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "- Аугментации: Легкие трансформации (повороты, флипы, масштабирование) улучшают обобщаемость модели.\n",
    "\n",
    "- Класс Dataset: В PyTorch или tf.data (TensorFlow) реализуйте пользовательский загрузчик, \n",
    "    который синхронно загружает изображения и соответствующие маски,\n",
    "    преобразует их в тензоры, нормализует и, при необходимости, применяет аугментации.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6be5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Кастомный датасет для задач сегментации лесных изображений.\n",
    "    \n",
    "    Пара «изображение-маска» подаётся синхронно и может проходить\n",
    "    через общие аугментации (Albumentations или иные).\n",
    "    \n",
    "    Параметры\n",
    "    ---------\n",
    "    root_img : str\n",
    "        Путь к директории с RGB-изображениями.\n",
    "    root_msk : str\n",
    "        Путь к директории с масками (одноканальные PNG/TIFF).\n",
    "    files : Sequence[str]\n",
    "        Список имён файлов (без пути), которые будут использоваться\n",
    "        в этом датасете (например, train или val-список).\n",
    "    aug : Optional[albumentations.core.composition.BaseCompose]\n",
    "        Пайплайн аугментаций Albumentations. Если None — аугментации\n",
    "        не применяются.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            image_dir: str,\n",
    "            mask_dir: str,\n",
    "            image_files: Sequence[str],\n",
    "            mask_files: Sequence[str],\n",
    "            transform: Optional[A.Compose]=None):\n",
    "        self.image_dir   = image_dir\n",
    "        self.mask_dir    = mask_dir\n",
    "        self.image_files = list(image_files)\n",
    "        self.mask_files  = list(mask_files)\n",
    "        self.transform   = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "    img_name = self.image_files[idx]\n",
    "    mask_name = self.mask_files[idx]\n",
    "    img_path = os.path.join(self.image, img_name)\n",
    "    msk_path = os.path.join(self.mask_dir, mask_name)\n",
    "\n",
    "    # 1. Load as NumPy arrays\n",
    "    img_np  = np.array(Image.open(img_path).convert(\"RGB\"), dtype=np.uint8)\n",
    "    mask_np = np.array(Image.open(msk_path).convert(\"L\"),  dtype=np.uint8)\n",
    "\n",
    "    # 2. Apply augmentations (if any)\n",
    "    if self.transform is not None:\n",
    "        augmented = self.transform(image=img_np, mask=mask_np)\n",
    "        img_aug, mask_aug = augmented[\"image\"], augmented[\"mask\"]\n",
    "        # Albumentations ToTensorV2 gives torch.Tensor; other transforms give numpy.ndarray\n",
    "        if isinstance(img_aug, np.ndarray):\n",
    "            img_tensor = torch.from_numpy(img_aug).permute(2,0,1).float() / 255.0\n",
    "        else:\n",
    "            img_tensor = img_aug.float() / 255.0\n",
    "\n",
    "        if isinstance(mask_aug, np.ndarray):\n",
    "            mask_tensor = torch.from_numpy(mask_aug).unsqueeze(0).float() / 255.0\n",
    "        else:\n",
    "            mask_tensor = mask_aug.float() / 255.0\n",
    "            if mask_tensor.ndim == 2:\n",
    "                mask_tensor = mask_tensor.unsqueeze(0)\n",
    "\n",
    "    else:\n",
    "        # No augmentations: convert both arrays to tensors\n",
    "        img_tensor = torch.from_numpy(img_np).permute(2,0,1).float() / 255.0\n",
    "        mask_tensor = torch.from_numpy(mask_np).unsqueeze(0).float() / 255.0\n",
    "\n",
    "    return img_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cc8d2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сложные аугментаций \n",
    "transform = Compose([\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomRotate90(p=0.5),\n",
    "    Affine(translate_percent=0.1, scale=(0.9,1.1), rotate=15, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d57b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# В Python и других программах путь с ~ не раскрывается автоматически, если передаётся как строка. \n",
    "# Его нужно явно преобразовать\n",
    "image_dir=os.path.expanduser('~/.cache/kagglehub/datasets/quadeer15sh/augmented-forest-segmentation/versions/2/Forest_Segmented/images')\n",
    "mask_dir=os.path.expanduser('~/.cache/kagglehub/datasets/quadeer15sh/augmented-forest-segmentation/versions/2/Forest_Segmented/masks')\n",
    "\n",
    "images = sorted(os.listdir(image_dir))\n",
    "masks = sorted(os.listdir(mask_dir))\n",
    "\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
    "                                            images, masks, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d604c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    image_files=train_imgs,\n",
    "    mask_files=train_masks,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = CustomDataset(\n",
    "    image_dir=image_dir,\n",
    "    mask_dir=mask_dir,\n",
    "    image_files=val_imgs,\n",
    "    mask_files=val_masks,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=8,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0,\n",
    "                          drop_last=False,\n",
    "                          pin_memory=False)\n",
    "valid_loader = DataLoader(val_dataset, \n",
    "                        batch_size=8,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        drop_last=False,\n",
    "                        pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63f51479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape, masks.shape)  # Например: torch.Size([8, 3, 256, 256]), torch.Size([8, 1, 256, 256])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9787d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 256, 256]) torch.Size([8, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for images, masks in valid_loader:\n",
    "    print(images.shape, masks.shape)  # Например: torch.Size([8, 3, 256, 256]), torch.Size([8, 1, 256, 256])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef28835",
   "metadata": {},
   "source": [
    "# функции метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfed476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(preds: torch.Tensor, masks: torch.Tensor, threshold: float = 0.5):\n",
    "    preds = (torch.sigmoid(preds) > threshold).float()\n",
    "    assert preds.shape == masks.shape, f\"Shape mismatch {preds.shape} vs {masks.shape}\"\n",
    "    intersection = (preds * masks).sum(dim=(1,2,3))\n",
    "    union        = ((preds + masks) > 0).float().sum(dim=(1,2,3))\n",
    "    return ((intersection + 1e-6) / (union + 1e-6)).mean().item()\n",
    "\n",
    "def pixel_accuracy(preds: torch.Tensor, masks: torch.Tensor, threshold: float = 0.5):\n",
    "    preds_bin = (torch.sigmoid(preds) > threshold).long()\n",
    "    masks_bin = (masks > threshold).long()\n",
    "    correct = (preds_bin == masks_bin).sum().item()\n",
    "    total   = masks_bin.numel()\n",
    "    return correct / total\n",
    "\n",
    "# масштабирование градиентов AMP\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474cdbde",
   "metadata": {},
   "source": [
    "# обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb96d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state: dict, checkpoint_dir: str, epoch: int):\n",
    "    \"\"\"\n",
    "    state: {\n",
    "      'epoch': текущий номер эпохи (int),\n",
    "      'model_state': model.state_dict(),\n",
    "      'optimizer_state': optimizer.state_dict(),\n",
    "      'scaler_state': scaler.state_dict()  # если используете GradScaler\n",
    "    }\n",
    "    checkpoint_dir: путь к директории для чекпойнтов\n",
    "    epoch: номер эпохи (используется для имени файла)\n",
    "    \"\"\"\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    filename = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch:03d}.pth')\n",
    "    torch.save(state, filename)\n",
    "    print(f'Checkpoint saved: {filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6377da",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '../checkpoints'\n",
    "\n",
    "def fit(model=model, train_loader=train_loader, valid_loader=valid_loader, \n",
    "        criterion=torch.nn.Module, optimizer=torch.optim.Optimizer, scaler=scaler, n_epochs=1):\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_iou': [], 'val_iou': []}\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # --- TRAIN ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for imgs, msks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} Train\"):\n",
    "            imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, msks)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # --- VALID ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_iou  = 0.0\n",
    "        with torch.no_grad():\n",
    "            for imgs, msks in tqdm(valid_loader, desc=f\"Epoch {epoch+1}/{n_epochs} Valid\"):\n",
    "                imgs, msks = imgs.to(DEVICE), msks.to(DEVICE)\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, msks)\n",
    "                val_loss += loss.item()\n",
    "                val_iou  += calculate_iou(outputs, msks)\n",
    "\n",
    "        avg_val_loss = val_loss / len(valid_loader)\n",
    "        avg_val_iou  = val_iou  / len(valid_loader)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['val_iou'].append(avg_val_iou)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_train_loss:.4f}, Val Loss={avg_val_loss:.4f}, Val IoU={avg_val_iou:.4f}\")\n",
    "\n",
    "    # сохраняем чекпойнт по окончании эпохи\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'scaler_state': scaler.state_dict(),           # если используете AMP\n",
    "    }, checkpoint_dir, epoch + 1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524ea2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48cbbc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 Train: 100%|██████████| 511/511 [02:28<00:00,  3.43it/s]\n",
      "Epoch 1/1 Valid: 100%|██████████| 128/128 [00:37<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss=nan, Val Loss=732.1196, Val IoU=0.0087\n",
      "Checkpoint saved: ../checkpoints/checkpoint_epoch_001.pth\n"
     ]
    }
   ],
   "source": [
    "history = fit(model=model, n_epochs=1, optimizer=optimizer, \n",
    "              train_loader=train_loader, valid_loader=valid_loader, \n",
    "              criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76091b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': [nan],\n",
       " 'val_loss': [-4696552.783203125],\n",
       " 'train_iou': [],\n",
       " 'val_iou': [140.76532119512558]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17444b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
